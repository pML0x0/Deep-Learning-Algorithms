{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise(object):\n",
    "    def __init__(self, mu, sigma=0.15, theta=0.2, dt= 1e-2, x0=None):\n",
    "\n",
    "        self.theta = theta \n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "            self.sigma * np.sqrt(self.dt) * np.random.normal(size = self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x_prev =  self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "  def __init__(self, buffer_size, batch_size, seed = 0):\n",
    "    self.buffer = []\n",
    "    self.max_size = buffer_size\n",
    "    self.batch_size = batch_size\n",
    "    self.random_generator = np.random.RandomState(seed)\n",
    "\n",
    "  def append(self, state, action, reward, terminal, next_state):\n",
    "    if(len(self.buffer) == self.max_size):\n",
    "      del self.buffer[0]\n",
    "\n",
    "    self.buffer.append([state, action, reward, terminal, next_state])\n",
    "\n",
    "  def sample(self):\n",
    "    batch = random.sample(self.buffer, self.batch_size)\n",
    "\n",
    "    state, action, reward, terminal, next_state = map(np.stack, zip(*batch))\n",
    "    \n",
    "    return state, action, reward, terminal, next_state\n",
    "\n",
    "  def get_buffer_size(self):\n",
    "    return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, beta, input_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir = 'tmp/ddpg'):\n",
    "        super(CriticNet, self).__init__()\n",
    "\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, name+'_ddpg')\n",
    "\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        #initializing the weights\n",
    "        f1 = 1/np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        #initializing the weights\n",
    "        f2 = 1/ np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
    "\n",
    "        self.action_value = nn.Linear(self.n_actions, self.fc2_dims)\n",
    "        \n",
    "        f3 = 0.003\n",
    "        self.q = nn.Linear(self.fc2_dims, 1)\n",
    "        T.nn.init.uniform_(self.q.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.q.bias.data, -f3, f3)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = beta)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_value = self.fc1(state)\n",
    "        state_value = self.bn1(state_value)\n",
    "        state_value = F.relu(state_value)\n",
    "\n",
    "        state_value = self.fc2(state_value)\n",
    "        state_value = self.bn2(state_value)\n",
    "\n",
    "        action_value = F.relu(self.action_value(action))\n",
    "        state_action_value = F.relu(T.add(state_value, action_value))\n",
    "        state_action_value = self.q(state_action_value)\n",
    "\n",
    "        return state_action_value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print(\".... saving checkpoint ....\")\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        print(\".... loading checkpoint ...\")\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir = 'tmp/ddpg'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, name+'_ddpg')\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        #Init weights\n",
    "        f1 = 1/np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
    "\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        #Init weights\n",
    "        f2 = 1/np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
    "\n",
    "        f3 = 0.003\n",
    "        self.mu = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        T.nn.init.uniform_(self.mu.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.mu.bias.data, -f3, f3)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = self.fc1(state)\n",
    "        state = self.bn1(state)\n",
    "        state = F.relu(state)\n",
    "        state = self.fc2(state)\n",
    "        state = self.bn2(state)\n",
    "        state = F.relu(state)\n",
    "        output = T.tanh(self.mu(state))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print(\".... saving checkpoint ....\")\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        print(\".... loading checkpoint ...\")\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, alpha, beta, input_dims, tau, env, \n",
    "               gamma = 0.99, n_actions = 2, max_size = 1000000, \n",
    "               layer1_size = 400, layer2_size = 300, batch_size = 64):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_size, batch_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.actor = ActorNetwork(alpha, input_dims, layer1_size,\n",
    "                                   layer2_size, n_actions=n_actions, name='Actor')\n",
    "\n",
    "        self.target_actor = ActorNetwork(alpha, input_dims, layer1_size,\n",
    "                                   layer2_size, n_actions=n_actions, name='TargetActor')\n",
    "\n",
    "\n",
    "        self.critic = CriticNet(beta, input_dims, layer1_size,\n",
    "                                   layer2_size, n_actions=n_actions, name='Critic')\n",
    "\n",
    "        self.target_critic = CriticNet(beta, input_dims, layer1_size,\n",
    "                                   layer2_size, n_actions=n_actions, name='TargetCritic')\n",
    "\n",
    "        self.noise = OUActionNoise(mu = np.zeros(n_actions))\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.actor.eval()\n",
    "     \n",
    "        observation = T.tensor([observation], dtype = T.float).to(self.actor.device)\n",
    "\n",
    "        mu = self.actor.forward(observation).to(self.actor.device)\n",
    "        mu_prime = mu + T.tensor(self.noise(),\n",
    "                                 dtype = T.float).to(self.actor.device)\n",
    "\n",
    "        self.actor.train()\n",
    "        return mu_prime.cpu().detach().numpy()[0]\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append(state, action, reward, new_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.get_buffer_size() < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        state, action, reward, next_state, terminal = self.memory.sample()\n",
    "\n",
    "        state_batch = T.FloatTensor(state).to(self.actor.device)\n",
    "        next_state_batch = T.FloatTensor(next_state).to(self.actor.device)\n",
    "        action_batch = T.FloatTensor(action).to(self.actor.device)\n",
    "        reward_batch = T.FloatTensor(reward).to(self.actor.device).unsqueeze(1)\n",
    "        terminal_batch = T.FloatTensor(terminal).to(self.actor.device).unsqueeze(1)\n",
    "\n",
    "        self.target_actor.eval()\n",
    "        self.target_critic.eval()\n",
    "        self.critic.eval()\n",
    "\n",
    "        target_actions = self.target_actor.forward(next_state_batch)\n",
    "        target_critic_value = self.target_critic.forward(next_state_batch, target_actions)\n",
    "        critic_value = self.critic.forward(state_batch, action_batch)\n",
    "\n",
    "        td = reward_batch + self.gamma * (1 - terminal_batch) * target_critic_value - critic_value\n",
    "      \n",
    "        critic_loss = (td ** 2).mean()\n",
    "        \n",
    "\n",
    "        self.critic.train()\n",
    "\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "        self.critic.eval()\n",
    "\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        mu = self.actor.forward(state_batch)\n",
    "        self.actor.train()\n",
    "        actor_loss = -self.critic.forward(state_batch, mu)\n",
    "        actor_loss = T.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def update_network_parameters(self, tau = None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        target_critict_dict = dict(target_critic_params)\n",
    "        target_actor_dict = dict(target_actor_params)\n",
    "\n",
    "\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau * critic_state_dict[name].clone() + (1-tau) * target_critict_dict[name].clone()\n",
    "\n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau * actor_state_dict[name].clone() + (1-tau) * target_actor_dict[name].clone()\n",
    "\n",
    "        self.target_actor.load_state_dict(actor_state_dict)\n",
    "\n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.target_actor.save_checkpoint()\n",
    "        self.target_critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.target_actor.load_checkpoint()\n",
    "        self.target_critic.load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "#from utils import plotLearning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00406663  0.00155556]\n",
      "[0.01016638 0.0348267 ]\n",
      "[0.03831946 0.02015677]\n",
      "[0.05270043 0.01790472]\n",
      "[0.05096894 0.02401411]\n",
      "[0.05319593 0.04601924]\n",
      "[0.06468512 0.0480765 ]\n",
      "[0.07140475 0.05320141]\n",
      "[0.09377005 0.05009263]\n",
      "[0.09836566 0.03728613]\n",
      "[0.05986036 0.0469351 ]\n",
      "[0.07268023 0.03573225]\n",
      "[0.10669285 0.01394102]\n",
      "[0.10719976 0.01102991]\n",
      "[0.12997803 0.03306344]\n",
      "[0.1322335  0.03884035]\n",
      "[0.11871859 0.00903658]\n",
      "[0.11321467 0.01125168]\n",
      "[0.13156402 0.02934746]\n",
      "[0.12552756 0.02475123]\n",
      "[0.10945971 0.00329004]\n",
      "[0.08376385 0.03266037]\n",
      "[0.07596058 0.02594471]\n",
      "[0.05687886 0.03728554]\n",
      "[0.03250423 0.03388443]\n",
      "[0.01913264 0.03969482]\n",
      "[0.01163173 0.02209189]\n",
      "[0.01139783 0.02867567]\n",
      "[0.01256338 0.03332879]\n",
      "[0.00320122 0.02799425]\n",
      "[-0.00685771  0.02255445]\n",
      "[-0.01909531 -0.00392472]\n",
      "[-0.01653371 -0.01052235]\n",
      "[-0.0411497  -0.00419524]\n",
      "[-0.05478102 -0.00399873]\n",
      "[-0.04381657 -0.00267074]\n",
      "[-0.02669076 -0.02179175]\n",
      "[-0.02064181 -0.03259953]\n",
      "[-0.03375666 -0.04183081]\n",
      "[-0.03853888 -0.04154197]\n",
      "[-0.05618752 -0.0285642 ]\n",
      "[-0.04935303 -0.05214061]\n",
      "[-0.02720039 -0.02410549]\n",
      "[-0.00973758 -0.02722335]\n",
      "[-0.02600972 -0.01174183]\n",
      "[-0.0322708   0.00617973]\n",
      "[-0.02933579  0.02040504]\n",
      "[-0.02424189  0.03052548]\n",
      "[-0.02433984  0.05680955]\n",
      "[-0.02267952  0.06230883]\n",
      "[0.00521439 0.04153332]\n",
      "[-0.01421072  0.05578957]\n",
      "[-0.03226943  0.08450175]\n",
      "[-0.03901255  0.07278118]\n",
      "[-0.01068746  0.09458143]\n",
      "[0.01674907 0.1077338 ]\n",
      "[0.00342928 0.13614479]\n",
      "[-0.00091036  0.1479531 ]\n",
      "[0.01267385 0.14509682]\n",
      "[0.02132557 0.15852529]\n",
      "[0.02658958 0.14168902]\n",
      "[0.03055033 0.16116026]\n",
      "[0.01957405 0.1583158 ]\n",
      "[0.01261293 0.18544897]\n",
      "[0.02532991 0.19603969]\n",
      "[0.01430254 0.2080954 ]\n",
      "[0.00500817 0.21247578]\n",
      "[-0.0029726   0.22646521]\n",
      "[0.0077034  0.22683665]\n",
      "[0.01621627 0.21296115]\n",
      "[-0.00373473  0.22121564]\n",
      "[0.00123544 0.23163734]\n",
      "[0.03967891 0.2457616 ]\n",
      "[0.02847852 0.2616047 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/padjei/miniconda3/envs/rlenv2/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00810658 0.26438794]\n",
      "[0.00857037 0.29075176]\n",
      "episode 0 score -140.17 100 game average -140.17\n",
      "[0.03503239 0.26397565]\n",
      "[0.03639638 0.2497655 ]\n",
      "[0.05551297 0.22893426]\n",
      "[0.04001869 0.21735688]\n",
      "[0.03408635 0.24088521]\n",
      "[0.04994725 0.23688455]\n",
      "[0.03214039 0.24384153]\n",
      "[0.01669678 0.21564244]\n",
      "[0.03318577 0.21550775]\n",
      "[0.04519006 0.21558575]\n",
      "[0.05534215 0.2009757 ]\n",
      "[0.03644572 0.20642301]\n",
      "[0.02024749 0.19117674]\n",
      "[0.00890482 0.18656763]\n",
      "[-0.00097256  0.16141744]\n",
      "[-0.01606302  0.12295166]\n",
      "[-0.01221024  0.09351998]\n",
      "[-0.03431176  0.08857978]\n",
      "[-0.05028898  0.10564658]\n",
      "[-0.07372098  0.10320713]\n",
      "[-0.07706809  0.07894905]\n",
      "[-0.07050628  0.06953307]\n",
      "[-0.05918932  0.07496325]\n",
      "[-0.02614803  0.08800506]\n",
      "[-0.03072566  0.07723214]\n",
      "[-0.012959    0.07976533]\n",
      "[-0.00179765  0.04813355]\n",
      "[-0.00050997  0.02968216]\n",
      "[0.00509045 0.02104415]\n",
      "[0.02021794 0.01917885]\n",
      "[0.03305578 0.00588426]\n",
      "[ 0.01905957 -0.00635937]\n",
      "[ 0.01808951 -0.00665125]\n",
      "[ 0.05076409 -0.01268488]\n",
      "[ 0.03423452 -0.02306581]\n",
      "[ 0.02381545 -0.02085544]\n",
      "[-0.002682   -0.02404884]\n",
      "[-0.00585831 -0.02507348]\n",
      "[-0.02092674 -0.03274874]\n",
      "[-0.04892917 -0.0438585 ]\n",
      "[-0.06411865 -0.04091373]\n",
      "[-0.08867419 -0.03224619]\n",
      "[-0.0737296  -0.06584021]\n",
      "[-0.07496651 -0.05739373]\n",
      "[-0.09214665 -0.06433818]\n",
      "[-0.10174145 -0.06898066]\n",
      "[-0.11370693 -0.09341531]\n",
      "[-0.1033032  -0.07559977]\n",
      "[-0.12185182 -0.09536571]\n",
      "[-0.11939653 -0.10134105]\n",
      "[-0.1214245 -0.1033152]\n",
      "[-0.1142208  -0.09008417]\n",
      "[-0.12724842 -0.10808317]\n",
      "[-0.15219809 -0.09611041]\n",
      "[-0.1703219  -0.10071763]\n",
      "[-0.17860289 -0.09823402]\n",
      "[-0.20615792 -0.09217889]\n",
      "[-0.1962204  -0.08749947]\n",
      "[-0.1983257  -0.08278057]\n",
      "[-0.18936403 -0.1211288 ]\n",
      "[-0.15688361 -0.11178371]\n",
      "[-0.16334021 -0.11384708]\n",
      "[-0.15237592 -0.11156294]\n",
      "[-0.17900953 -0.07645843]\n",
      "[-0.17623577 -0.05714978]\n",
      "[-0.18162501 -0.03032016]\n",
      "[-0.17174786 -0.01741491]\n",
      "[-0.18121085  0.00460465]\n",
      "[-0.16411236  0.02801794]\n",
      "[-0.16641538  0.02461731]\n",
      "[-0.12450773  0.01261846]\n",
      "[-0.11901677  0.03374416]\n",
      "[-0.10986972  0.04672935]\n",
      "[-0.10796197  0.05681664]\n",
      "[-0.11953092  0.0865998 ]\n",
      "[-0.11298179  0.0814337 ]\n",
      "[-0.09448631  0.07971974]\n",
      "[-0.10587509  0.06483229]\n",
      "[-0.0865989   0.06785105]\n",
      "[-0.07957292  0.08945438]\n",
      "[-0.08741044  0.0838894 ]\n",
      "[-0.08407576  0.09062514]\n",
      "[-0.0757051   0.09183756]\n",
      "[-0.06768553  0.09565677]\n",
      "[-0.06942759  0.08861037]\n",
      "[-0.05634905  0.08012439]\n",
      "[-0.07822855  0.05927773]\n",
      "[-0.07299126  0.09679837]\n",
      "episode 1 score -199.51 100 game average -169.84\n",
      "[0.01147883 0.17471923]\n",
      "[0.03155258 0.16322555]\n",
      "[0.05188021 0.151755  ]\n",
      "[0.02080965 0.16709898]\n",
      "[0.00315109 0.18016608]\n",
      "[0.00096534 0.21135059]\n",
      "[0.02536422 0.21071854]\n",
      "[0.02322574 0.198564  ]\n",
      "[0.02451707 0.2001647 ]\n",
      "[0.02680399 0.20814148]\n",
      "[0.04347172 0.21869284]\n",
      "[0.0402946 0.2020315]\n",
      "[0.06853582 0.1963308 ]\n",
      "[0.06646752 0.19251896]\n",
      "[0.04645486 0.18901548]\n",
      "[0.04659951 0.20191275]\n",
      "[0.06477444 0.2055813 ]\n",
      "[0.08573   0.2142449]\n",
      "[0.09167108 0.22031064]\n",
      "[0.09524906 0.21823016]\n",
      "[0.09705499 0.20528883]\n",
      "[0.10630772 0.19440602]\n",
      "[0.12313499 0.19479576]\n",
      "[0.1281417  0.20646632]\n",
      "[0.14170757 0.18719913]\n",
      "[0.1416122  0.20480415]\n",
      "[0.11916652 0.21515676]\n",
      "[0.08873707 0.20460165]\n",
      "[0.09257053 0.18380788]\n",
      "[0.08124048 0.1650311 ]\n",
      "[0.10970768 0.17003804]\n",
      "[0.12184149 0.16821575]\n",
      "[0.12091632 0.1448284 ]\n",
      "[0.12094101 0.13329822]\n",
      "[0.13801679 0.14994225]\n",
      "[0.16402349 0.16214195]\n",
      "[0.15879676 0.14492805]\n",
      "[0.1743774  0.15025374]\n",
      "[0.15203524 0.15591222]\n",
      "[0.16870682 0.15796186]\n",
      "[0.17065461 0.1477425 ]\n",
      "[0.15428852 0.16311422]\n",
      "[0.15418048 0.16301544]\n",
      "[0.1852554 0.1774231]\n",
      "[0.19648974 0.1845679 ]\n",
      "[0.22435237 0.21481471]\n",
      "[0.24379802 0.22802334]\n",
      "[0.21817553 0.23451614]\n",
      "[0.22258182 0.24058524]\n",
      "[0.24269083 0.2033143 ]\n",
      "[0.24084759 0.21126772]\n",
      "[0.24050663 0.19348921]\n",
      "[0.2599896  0.20076868]\n",
      "[0.25186002 0.20850334]\n",
      "[0.27130187 0.21200131]\n",
      "[0.2898749  0.21605065]\n",
      "[0.2904489 0.2018897]\n",
      "[0.2957982  0.20846306]\n",
      "[0.31552044 0.21083103]\n",
      "[0.32858774 0.20075896]\n",
      "[0.3225054  0.20348606]\n",
      "[0.31230187 0.22057754]\n",
      "[0.32408622 0.22890468]\n",
      "[0.33393562 0.26318976]\n",
      "[0.33235765 0.23440316]\n",
      "[0.34076583 0.23758426]\n",
      "[0.3382189  0.22609614]\n",
      "[0.3365409 0.2310598]\n",
      "[0.3334845  0.26817325]\n",
      "[0.3232862  0.25629038]\n",
      "[0.29311922 0.2594634 ]\n",
      "[0.267043  0.2502923]\n",
      "[0.26682374 0.23825681]\n",
      "[0.29711637 0.25221354]\n",
      "[0.3202535 0.2376652]\n",
      "[0.34020317 0.24360904]\n",
      "[0.3548218  0.25748938]\n",
      "[0.3433696  0.24851036]\n",
      "[0.35776126 0.20508917]\n",
      "[0.3817627  0.20941065]\n",
      "[0.381336   0.20778738]\n",
      "[0.36569965 0.19690683]\n",
      "[0.38035452 0.2092366 ]\n",
      "[0.41855526 0.20015684]\n",
      "[0.4044784  0.18264851]\n",
      "[0.38534838 0.17177673]\n",
      "[0.38636735 0.16126642]\n",
      "[0.3978802  0.14747538]\n",
      "[0.38561976 0.12722054]\n",
      "[0.3536784  0.09168899]\n",
      "[0.32716715 0.0980638 ]\n",
      "episode 2 score -293.52 100 game average -211.07\n",
      "[0.44853014 0.246068  ]\n",
      "[0.42691183 0.21084347]\n",
      "[0.41533238 0.20782323]\n",
      "[0.42058256 0.21735029]\n",
      "[0.4227243 0.2373692]\n",
      "[0.4167475  0.21707511]\n",
      "[0.38829616 0.19652225]\n",
      "[0.38926286 0.17616242]\n",
      "[0.39941964 0.1895098 ]\n",
      "[0.3604208  0.19490758]\n",
      "[0.32242405 0.19274162]\n",
      "[0.33597383 0.18550044]\n",
      "[0.31141645 0.16813959]\n",
      "[0.30211478 0.15684612]\n",
      "[0.29236847 0.14556238]\n",
      "[0.28859285 0.11777781]\n",
      "[0.27648678 0.13003828]\n",
      "[0.2625306  0.11312047]\n",
      "[0.2707448  0.06483775]\n",
      "[0.27858794 0.04484096]\n",
      "[0.27114108 0.0357728 ]\n",
      "[0.27530038 0.02173425]\n",
      "[0.25593644 0.03229747]\n",
      "[0.21160007 0.02514656]\n",
      "[0.18290988 0.00830962]\n",
      "[0.2045339  0.00359435]\n",
      "[ 0.20052212 -0.02504645]\n",
      "[ 0.18307003 -0.02262063]\n",
      "[ 0.18282488 -0.01266833]\n",
      "[ 0.1777404  -0.00316817]\n",
      "[ 0.18083647 -0.02777706]\n",
      "[ 0.17597389 -0.0133325 ]\n",
      "[ 0.17964458 -0.01525585]\n",
      "[ 0.16301557 -0.02812485]\n",
      "[ 0.15698114 -0.00714169]\n",
      "[ 0.12175879 -0.0131051 ]\n",
      "[0.09858653 0.00261482]\n",
      "[ 0.08501619 -0.00133739]\n",
      "[0.07557654 0.00782694]\n",
      "[ 0.06239101 -0.03550611]\n",
      "[ 0.06208381 -0.02702966]\n",
      "[ 0.04882288 -0.04234666]\n",
      "[ 0.04447687 -0.03650546]\n",
      "[ 0.02614525 -0.0408886 ]\n",
      "[ 3.7610531e-05 -4.5476034e-02]\n",
      "[-0.0355626  -0.04198339]\n",
      "[-0.06279323 -0.04167889]\n",
      "[-0.09580374 -0.05413214]\n",
      "[-0.10804725 -0.08309618]\n",
      "[-0.11345419 -0.09253541]\n",
      "[-0.14531848 -0.11578473]\n",
      "[-0.13900265 -0.10556661]\n",
      "[-0.14170167 -0.10234141]\n",
      "[-0.13704744 -0.09976304]\n",
      "[-0.1151289  -0.12399793]\n",
      "[-0.10167354 -0.1303081 ]\n",
      "[-0.10029477 -0.13423204]\n",
      "[-0.10037896 -0.1512048 ]\n",
      "[-0.0921706  -0.12381659]\n",
      "[-0.08723977 -0.12070466]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m new_state, reward, done, info, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(act)\n\u001b[1;32m     18\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(obs,act,reward,new_state, \u001b[38;5;28mint\u001b[39m(done))\n\u001b[0;32m---> 19\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     22\u001b[0m obs \u001b[38;5;241m=\u001b[39m new_state\n",
      "Cell \u001b[0;32mIn[95], line 75\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mforward(state_batch)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv2/lib/python3.9/site-packages/torch/optim/optimizer.py:279\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    277\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m foreach \u001b[38;5;129;01mor\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse):\n\u001b[0;32m--> 279\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     per_device_and_dtype_grads[p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdevice][p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype]\u001b[38;5;241m.\u001b[39mappend(p\u001b[38;5;241m.\u001b[39mgrad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "agent = Agent(alpha = 0.000025, beta= 0.00025, input_dims = [8], tau=0.001, env = env, batch_size=64, layer1_size=400, layer2_size=300, n_actions = 2)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "score_history = []\n",
    "\n",
    "for i in range(1500):\n",
    "    done = False\n",
    "    score = 0\n",
    "    obs = env.reset()[0]\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        act = agent.choose_action(obs)\n",
    "        print(act)\n",
    "        new_state, reward, done, info, _ = env.step(act)\n",
    "        agent.remember(obs,act,reward,new_state, int(done))\n",
    "        agent.learn()\n",
    "\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "\n",
    "    score_history.append(score)\n",
    "    print('episode', i, 'score %.2f' % score, '100 game average %.2f' % np.mean(score_history[-100:]))\n",
    "\n",
    "    # if i % 25 == 0:\n",
    "    #     agent.save_models()\n",
    "\n",
    "# filename = 'lunar-lander.png'\n",
    "# plotLearning(score_history, filename, window=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
