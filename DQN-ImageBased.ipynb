{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# import gym\n",
    "# from gym.envs.box2d.lunar_lander import LunarLander\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "  def __init__(self, buffer_size, batch_size, seed = 0):\n",
    "    self.buffer = []\n",
    "    self.max_size = buffer_size\n",
    "    self.batch_size = batch_size\n",
    "    self.random_generator = np.random.RandomState(seed)\n",
    "\n",
    "  def append(self, state, action, reward, terminal, next_state):\n",
    "    if(len(self.buffer) == self.max_size):\n",
    "      del self.buffer[0]\n",
    "\n",
    "    self.buffer.append([state, action, reward, terminal, next_state])\n",
    "\n",
    "  def sample(self):\n",
    "    batch = random.sample(self.buffer, self.batch_size)\n",
    "\n",
    "    state, action, reward, terminal, next_state = map(np.stack, zip(*batch))\n",
    "    \n",
    "    return state, action, reward, terminal, next_state\n",
    "\n",
    "  def get_buffer_size(self):\n",
    "    return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# %%\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(input_channels, 16, kernel_size =3, stride =1)\n",
    "\n",
    "        def size_linear_unit(size, kernel_size = 3, stride = 1):\n",
    "            return (size - (kernel_size - 1) - 1)//stride + 1\n",
    "\n",
    "        num_hidden_unit = size_linear_unit(10) * size_linear_unit(10) * 16\n",
    "\n",
    "        self.fc_hidden1 = nn.Linear(num_hidden_unit, 128)\n",
    "\n",
    "        self.output = nn.Linear(128,num_actions)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = state.permute(0,3,1,2)\n",
    "        \n",
    "        x1 = F.relu(self.conv(state))\n",
    "        x1 = F.relu(self.fc_hidden1( x1.reshape(x1.size(0), -1) ))\n",
    "        x1 = self.output(x1)\n",
    "        \n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAD_MOMENTUM = 0.95\n",
    "MIN_SQUARED_GRAD = 0.01\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, observation_space = 10, channels = 4, action_space = 4, lr = 1e-3, \n",
    "                 gamma = 0.99, epsilon = 0.91, annealing_coefficient =  0.9999, tau = 1e-3, seed = 1):\n",
    "        \n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.observation_space = observation_space #In minitar it's always 10\n",
    "        self.channels = channels\n",
    "        self.actions = action_space\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.tau = tau\n",
    "        self.annealing_coefficient = annealing_coefficient\n",
    "\n",
    "\n",
    "        self.device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        self.Q_Net = QNetwork(self.channels, self.actions).to(self.device)\n",
    "        \n",
    "        self.target_Q_Net = QNetwork(self.channels, self.actions).to(self.device)\n",
    "        \n",
    "        for target_param, param in zip( self.target_Q_Net.parameters(), self.Q_Net.parameters()):\n",
    "            target_param.data.copy_( param.data * self.tau + (1-self.tau) * target_param.data)\n",
    "        \n",
    "        self.joint_optimizer = RMSprop(self.Q_Net.parameters(), lr=self.lr, alpha=GRAD_MOMENTUM, centered=True, eps=MIN_SQUARED_GRAD)\n",
    "\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        self.buffer = ReplayBuffer(100000, self.batch_size)\n",
    "\n",
    "    def take_action(self, state):\n",
    " \n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        \n",
    "        actions_pred = self.Q_Net.forward(state)\n",
    "\n",
    "        actions_pred = actions_pred.detach().cpu().numpy()[0]\n",
    "        \n",
    "        softmax_q = np.exp(actions_pred - np.max(actions_pred))/np.sum(np.exp(actions_pred - np.max(actions_pred)))\n",
    "        \n",
    "        \n",
    "        if self.epsilon > self.rng.random():\n",
    "            action = self.rng.choice(self.actions)\n",
    "            #aindex = np.where(action == self.actions)[0][0]\n",
    "            \n",
    "            return action, softmax_q[action], softmax_q\n",
    "        else:\n",
    "            action = np.argmax(actions_pred)\n",
    "\n",
    "            return action, softmax_q[action], softmax_q\n",
    "        \n",
    "    def update(self):\n",
    "        state, action, reward, terminal, next_state = self.buffer.sample()\n",
    "\n",
    "\n",
    "        state_batch = torch.FloatTensor(state).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action).to(self.device).long().unsqueeze(1)\n",
    "        reward_batch = torch.FloatTensor(reward).to(self.device).unsqueeze(1)\n",
    "        terminal = torch.FloatTensor(terminal).to(self.device).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        qs = self.Q_Net.forward(state_batch)\n",
    "        q = torch.gather(qs, 1, action_batch)\n",
    "\n",
    "\n",
    "        next_qs = self.target_Q_Net.forward(next_state_batch)\n",
    "\n",
    "        next_q = torch.max(next_qs, 1, True)[0]\n",
    "\n",
    "        td = reward_batch + self.gamma * (1 - terminal) * next_q - q\n",
    "        #target = reward_batch + self.gamma * (1 - terminal) * next_q\n",
    "        #qf_loss = F.mse_loss(q, target)\n",
    "        qf_loss = (td ** 2).mean()\n",
    "\n",
    "        self.joint_optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.joint_optimizer.step()\n",
    "\n",
    "\n",
    "        for target_param, param in zip( self.target_Q_Net.parameters(), self.Q_Net.parameters()):\n",
    "            target_param.data.copy_( param.data * self.tau + (1-self.tau) * target_param.data)\n",
    "\n",
    "        if self.epsilon > 0.1:\n",
    "            self.epsilon *= self.annealing_coefficient\n",
    "\n",
    "\n",
    "    def set_RDG_seed(self, seed):\n",
    "        self.rng = np.random.default_rng(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m frame \u001b[38;5;241m<\u001b[39m FRAMES:\n\u001b[0;32m---> 25\u001b[0m     action, q, allQ \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     new_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     29\u001b[0m     G \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m, in \u001b[0;36mAgent.take_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     39\u001b[0m actions_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_Net\u001b[38;5;241m.\u001b[39mforward(state)\n\u001b[1;32m     41\u001b[0m actions_pred \u001b[38;5;241m=\u001b[39m actions_pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 43\u001b[0m softmax_q \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(actions_pred \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(actions_pred))\u001b[38;5;241m/\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng\u001b[38;5;241m.\u001b[39mrandom():\n\u001b[1;32m     47\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv2/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv2/lib/python3.9/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MinAtar/Asterix-v1\")\n",
    "\n",
    "FRAMES = 3000000\n",
    "\n",
    "returns_across_seeds = []\n",
    "\n",
    "annealing_rate = (0.1/0.91)**(1/100000)\n",
    "\n",
    "for seed in range(1):\n",
    "\n",
    "    agent = Agent(action_space = env.action_space.n, lr = 3e-4, annealing_coefficient=annealing_rate, tau=1e-3, seed=seed)\n",
    "        \n",
    "    return_per_episode = []\n",
    "\n",
    "    frame = 0\n",
    "    \n",
    "    while frame < FRAMES: #episodes\n",
    "\n",
    "        state = env.reset()[0]\n",
    "        G = 0\n",
    "        done = 0\n",
    "\n",
    "        while not done and frame < FRAMES:\n",
    "\n",
    "            action, q, allQ = agent.take_action(state)\n",
    "\n",
    "            new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            G += reward\n",
    "\n",
    "            agent.buffer.append(state, action, reward, done, new_state)\n",
    "\n",
    "            if (agent.buffer.get_buffer_size() > agent.buffer.batch_size):\n",
    "                agent.update()\n",
    "            \n",
    "            state = new_state\n",
    "\n",
    "            frame+=1\n",
    "        \n",
    "        return_per_episode.append(G)\n",
    "\n",
    "    returns_across_seeds.append(return_per_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
